# AI DEFINITIONS

### ai
Systems capable of preforming tasks that requires human ntelligence. Ml is within the center of ai

### ml
Consists in the development of algorithms and models that enable the computers to learn and make predictions or make decisions based on data.

#### types of ml
- Supervised learning: Algorithms learn from labeled training data, aiming to predict outcomes for new inputs.

- Unsupervised learning: Algorithms identify patterns in data without needing labeled responses, often used for clustering and association.

- Reinforcement learning: Models learn to make sequences of decisions by receiving feedback on the actions' effectiveness.

#### algorithms
Linear regression, decision trees, random forests, neural networks, support vector machines, k-means clustering, and hierarchical clustering.

#### data handling
Ml requires: robust data preprocessing, normalization, handling missing data. and feature selection to improve accuracy.

### deep learning
DL is an advanced branch of ML that uses artificial neural networks with multiple layers, known as deep neural networks. 

This technique is particularly effective in areas such as image recognition, natural language processing (NLP), and speech recognition.

# neural networks
Neural networks (NN) are a cornerstone of AI. They are particularly effective in pattern recognition and data interpretation tasks.

### nlp
Focuses on the interaction between computers and humans through natural language. 

Key techniques in NLP include syntax tree parsing, entity recognition, and sentiment analysis, among others. 

NLP is used in a variety of applications, such as automated chatbots, translation services, email filtering, and voice-activated global position systems (GPS).

### transformers
Transformers use a mechanism known as self-attention to weigh the importance of each word in a sentence, regardless of its position. transformers process all words or tokens in parallel, which significantly increases efficiency and performance.

### generative pre-trained transformers
These models are first pre-trained on a diverse range of internet text to develop a broad understanding of language structure and context. Once pre-trained, GPT models can be fine-tuned on specific tasks such as translation, question-answering, and summarization, enhancing their applicability across various domains. 

### Tokenization, Word2vec, and BERT
- Tokenization in NLP involves splitting text into smaller units known as tokens, which can be words, characters, or subwords. 
- Word2vec, developed by researchers at Google, is a technique that embeds words into numerical vectors.
- Developed also by Google, BERT incorporates a transformer architecture that processes words in relation to all the other words in a sentence, rather than one-by-one in order. 